{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework - Neural networks - Part B (45 points)\n",
    "## Gradient descent for simple two and three layer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by *Brenden Lake* and *Todd Gureckis*  \n",
    "Computational Cognitive Modeling  \n",
    "NYU class webpage: https://brendenlake.github.io/CCM-site/  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "This homework is due before midnight on Monday, Feb. 14, 2022.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of this assignment implements the gradient descent algorithm for a simple artificial neuron. The second part implements backpropagation for a simple network with one hidden unit.\n",
    "\n",
    "In the first part, the neuron will learn to compute logical OR. The neuron model and logical OR are shown below, for inputs $x_0$ and $x_1$ and target output $y$.\n",
    "\n",
    "<img src=\"images/nn_OR.jpeg\" style=\"width: 350px;\"/>\n",
    "\n",
    "This assignment requires some basic PyTorch knowledge. You can review your notes from lab and also two basic [PyTorch tutorials](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html), \"What is PyTorch?\" and \"Autograd\", which should have the basics you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/changhyunlee/opt/anaconda3/lib/python3.8/site-packages (1.10.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/changhyunlee/opt/anaconda3/lib/python3.8/site-packages (from torch) (3.10.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create `torch.tensor` objects for representing the data matrix `D` with targets `Y_or` (for the logical OR function). Each row of `D` is a different data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "D = np.zeros((4,2),dtype=float)\n",
    "D[0,:] = [0.,0.]\n",
    "D[1,:] = [0.,1.]\n",
    "D[2,:] = [1.,0.]\n",
    "D[3,:] = [1.,1.]\n",
    "D = torch.tensor(D,dtype=torch.float)\n",
    "Y_or = torch.tensor([0.,1.,1.,1.])\n",
    "N = D.shape[0] # number of input patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The artificial neuron operates as follows. Given an input vector $x$, the net input ($\\textbf{net}$) to the neuron is computed as follows\n",
    "\n",
    "$$ \\textbf{net} = \\sum_i x_i w_i + b,$$\n",
    "\n",
    "for weights $w_i$ and bias $b$. The activation function $g(\\textbf{net})$ is the logistic function\n",
    "\n",
    "$$ g(\\textbf{net}) = \\frac{1}{1+e^{-\\textbf{net}}},$$\n",
    "\n",
    "which is used to compute the predicted output $\\hat{y} = g(\\textbf{net})$. Finally, the loss (squared error) for a particular pattern $x$ is defined as \n",
    "\n",
    "$$ E(w,b) = (\\hat{y}-y)^2,$$\n",
    "\n",
    "where the target output is $y$. **Your main task is to manually compute the gradients of the loss $E$ with respect to the neuron parameters:**\n",
    "\n",
    "$$\\frac{\\partial E(w,b)}{\\partial w}, \\frac{\\partial E(w,b)}{\\partial b}.$$\n",
    "\n",
    "By manually, we mean to program the gradient computation directly, using the formulas discussed in class. This is in contrast to using PyTorch's `autograd` (Automatric differentiation) that computes the gradient automatically, as discussed in class, lab, and in the PyTorch tutorial (e.g., `loss.backward()`). First, let's write the activation function and the loss in PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_logistic(net):\n",
    "    return 1. / (1.+torch.exp(-net))\n",
    "\n",
    "def loss(yhat,y):\n",
    "    return (yhat-y)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll also write two functions for examining the internal operations of the neuron, and the gradients of its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_forward(x,yhat,y):\n",
    "    # Examine network's prediction for input x\n",
    "    print(' Input: ',end='')\n",
    "    print(x.numpy())\n",
    "    print(' Output: ' + str(round(yhat.item(),3)))\n",
    "    print(' Target: ' + str(y.item()))\n",
    "\n",
    "def print_grad(grad_w,grad_b):\n",
    "    # Examine gradients\n",
    "    print('  d_loss / d_w = ',end='')\n",
    "    print(grad_w)\n",
    "    print('  d_loss / d_b = ',end='')\n",
    "    print(grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's dive in and begin the implementation of stochastic gradient descent. We'll initialize our parameters $w$ and $b$ randomly, and proceed through a series of epochs of training. Each epoch involves visiting the four training patterns in random order, and updating the parameters after each presentation of an input pattern.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 1 (10 points) </h3>\n",
    "<br>\n",
    "In the code below, fill in code to manually compute the gradient in closed form.\n",
    "    <ul>\n",
    "        <li>See lecture slides for the equation for the gradient for the weights w.</li>\n",
    "        <li>Derive (or reason) to get the equation for the gradient for bias b.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 2 (5 points) </h3>\n",
    "<br>\n",
    "In the code below, fill in code for the weight and bias update rule for gradient descent.\n",
    "</div>\n",
    "\n",
    "After completing the code, run it to compare **your gradients** with the **ground-truth computed by PyTorch.** (There may be small differences that you shouldn't worry about, e.g. within 1e-6). Also, you can check the neuron's performance at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute the gradient manually\n",
      " Input: [0. 1.]\n",
      " Output: 0.757\n",
      " Target: 1.0\n",
      "  d_loss / d_w = [-0.         -0.08914106]\n",
      "  d_loss / d_b = [-0.08914106]\n",
      "Compute the gradient using PyTorch .backward()\n",
      "  d_loss / d_w = [-0.         -0.08914107]\n",
      "  d_loss / d_b = [-0.08914107]\n",
      "\n",
      "Compute the gradient manually\n",
      " Input: [0. 0.]\n",
      " Output: 0.854\n",
      " Target: 0.0\n",
      "  d_loss / d_w = [0. 0.]\n",
      "  d_loss / d_b = [0.21304512]\n",
      "Compute the gradient using PyTorch .backward()\n",
      "  d_loss / d_w = [0. 0.]\n",
      "  d_loss / d_b = [0.21304509]\n",
      "\n",
      "Compute the gradient manually\n",
      " Input: [1. 0.]\n",
      " Output: 0.719\n",
      " Target: 1.0\n",
      "  d_loss / d_w = [-0.11326589 -0.        ]\n",
      "  d_loss / d_b = [-0.11326589]\n",
      "Compute the gradient using PyTorch .backward()\n",
      "  d_loss / d_w = [-0.11326589  0.        ]\n",
      "  d_loss / d_b = [-0.11326589]\n",
      "\n",
      "Compute the gradient manually\n",
      " Input: [1. 1.]\n",
      " Output: 0.583\n",
      " Target: 1.0\n",
      "  d_loss / d_w = [-0.20279284 -0.20279284]\n",
      "  d_loss / d_b = [-0.20279284]\n",
      "Compute the gradient using PyTorch .backward()\n",
      "  d_loss / d_w = [-0.20279285 -0.20279285]\n",
      "  d_loss / d_b = [-0.20279285]\n",
      "\n",
      "epoch 0; error=1.041\n",
      "epoch 50; error=0.851\n",
      "epoch 100; error=0.792\n",
      "epoch 150; error=0.728\n",
      "epoch 200; error=0.649\n",
      "epoch 250; error=0.56\n",
      "epoch 300; error=0.474\n",
      "epoch 350; error=0.4\n",
      "epoch 400; error=0.34\n",
      "epoch 450; error=0.293\n",
      "epoch 500; error=0.255\n",
      "epoch 550; error=0.224\n",
      "epoch 600; error=0.199\n",
      "epoch 650; error=0.178\n",
      "epoch 700; error=0.16\n",
      "epoch 750; error=0.146\n",
      "epoch 800; error=0.133\n",
      "epoch 850; error=0.122\n",
      "epoch 900; error=0.113\n",
      "epoch 950; error=0.105\n",
      "epoch 1000; error=0.097\n",
      "epoch 1050; error=0.091\n",
      "epoch 1100; error=0.085\n",
      "epoch 1150; error=0.08\n",
      "epoch 1200; error=0.076\n",
      "epoch 1250; error=0.071\n",
      "epoch 1300; error=0.068\n",
      "epoch 1350; error=0.064\n",
      "epoch 1400; error=0.061\n",
      "epoch 1450; error=0.058\n",
      "epoch 1500; error=0.056\n",
      "epoch 1550; error=0.053\n",
      "epoch 1600; error=0.051\n",
      "epoch 1650; error=0.049\n",
      "epoch 1700; error=0.047\n",
      "epoch 1750; error=0.045\n",
      "epoch 1800; error=0.044\n",
      "epoch 1850; error=0.042\n",
      "epoch 1900; error=0.041\n",
      "epoch 1950; error=0.039\n",
      "epoch 2000; error=0.038\n",
      "epoch 2050; error=0.037\n",
      "epoch 2100; error=0.036\n",
      "epoch 2150; error=0.035\n",
      "epoch 2200; error=0.034\n",
      "epoch 2250; error=0.033\n",
      "epoch 2300; error=0.032\n",
      "epoch 2350; error=0.031\n",
      "epoch 2400; error=0.03\n",
      "epoch 2450; error=0.029\n",
      "epoch 2500; error=0.029\n",
      "epoch 2550; error=0.028\n",
      "epoch 2600; error=0.027\n",
      "epoch 2650; error=0.027\n",
      "epoch 2700; error=0.026\n",
      "epoch 2750; error=0.025\n",
      "epoch 2800; error=0.025\n",
      "epoch 2850; error=0.024\n",
      "epoch 2900; error=0.024\n",
      "epoch 2950; error=0.023\n",
      "epoch 3000; error=0.023\n",
      "epoch 3050; error=0.022\n",
      "epoch 3100; error=0.022\n",
      "epoch 3150; error=0.021\n",
      "epoch 3200; error=0.021\n",
      "epoch 3250; error=0.021\n",
      "epoch 3300; error=0.02\n",
      "epoch 3350; error=0.02\n",
      "epoch 3400; error=0.019\n",
      "epoch 3450; error=0.019\n",
      "epoch 3500; error=0.019\n",
      "epoch 3550; error=0.018\n",
      "epoch 3600; error=0.018\n",
      "epoch 3650; error=0.018\n",
      "epoch 3700; error=0.018\n",
      "epoch 3750; error=0.017\n",
      "epoch 3800; error=0.017\n",
      "epoch 3850; error=0.017\n",
      "epoch 3900; error=0.016\n",
      "epoch 3950; error=0.016\n",
      "epoch 4000; error=0.016\n",
      "epoch 4050; error=0.016\n",
      "epoch 4100; error=0.016\n",
      "epoch 4150; error=0.015\n",
      "epoch 4200; error=0.015\n",
      "epoch 4250; error=0.015\n",
      "epoch 4300; error=0.015\n",
      "epoch 4350; error=0.014\n",
      "epoch 4400; error=0.014\n",
      "epoch 4450; error=0.014\n",
      "epoch 4500; error=0.014\n",
      "epoch 4550; error=0.014\n",
      "epoch 4600; error=0.014\n",
      "epoch 4650; error=0.013\n",
      "epoch 4700; error=0.013\n",
      "epoch 4750; error=0.013\n",
      "epoch 4800; error=0.013\n",
      "epoch 4850; error=0.013\n",
      "epoch 4900; error=0.013\n",
      "epoch 4950; error=0.012\n",
      "Final result:\n",
      " Input: [1. 1.]\n",
      " Output: 1.0\n",
      " Target: 1.0\n",
      "\n",
      "Final result:\n",
      " Input: [0. 1.]\n",
      " Output: 0.948\n",
      " Target: 1.0\n",
      "\n",
      "Final result:\n",
      " Input: [1. 0.]\n",
      " Output: 0.948\n",
      " Target: 1.0\n",
      "\n",
      "Final result:\n",
      " Input: [0. 0.]\n",
      " Output: 0.083\n",
      " Target: 0.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "#     Although you will implement gradient descent manually, let's set requires_grad=True\n",
    "#     anyway so PyTorch will track the gradient too, and we can compare your gradient with PyTorch's.\n",
    "w = torch.randn(2) # [size 2] tensor\n",
    "b = torch.randn(1) # [size 1] tensor\n",
    "w.requires_grad = True\n",
    "b.requires_grad = True\n",
    "\n",
    "alpha = 0.05 # learning rate\n",
    "nepochs = 5000 # number of epochs\n",
    "\n",
    "track_error = []\n",
    "verbose = True\n",
    "for e in range(nepochs): # for each epoch\n",
    "    error_epoch = 0. # sum loss across the epoch\n",
    "    perm = np.random.permutation(N)\n",
    "    for p in perm: # visit data points in random order\n",
    "        x = D[p,:] # input pattern\n",
    "        \n",
    "        # compute output of neuron\n",
    "        net = torch.dot(x,w)+b\n",
    "        yhat = g_logistic(net)\n",
    "        \n",
    "        # compute loss\n",
    "        y = Y_or[p]\n",
    "        myloss = loss(yhat,y)\n",
    "        error_epoch += myloss.item()\n",
    "        \n",
    "        # print output if this is the last epoch\n",
    "        if (e == nepochs-1):\n",
    "            print(\"Final result:\")\n",
    "            print_forward(x,yhat,y)\n",
    "            print(\"\")\n",
    "\n",
    "        # Compute the gradient manually\n",
    "        if verbose:\n",
    "            print('Compute the gradient manually')\n",
    "            print_forward(x,yhat,y)\n",
    "        with torch.no_grad():\n",
    "            # TODO : YOUR GRADIENT CODE GOES HERE\n",
    "            #  two lines of the form\n",
    "            #    w_grad = ...    ([size 2] PyTorch tensor)\n",
    "            #    b_grad = ...    ([size 1] PyTorch tensor)\n",
    "            #  make sure to inclose your code in the \"with torch.no_grad()\" wrapper,\n",
    "            #   otherwise PyTorch will try to track the \"gradient\" of the gradient computation, which we don't want.         \n",
    "            \n",
    "            # Part 1\n",
    "            w_grad = 2 * (yhat - y) * g_logistic(net) * (1 - g_logistic(net)) * x\n",
    "            b_grad = 2 * (yhat - y) * g_logistic(net) * (1 - g_logistic(net)) * 1\n",
    "            \n",
    "            #raise Exception('Replace with your code.')                      \n",
    "        if verbose: print_grad(w_grad.numpy(),b_grad.numpy())\n",
    "\n",
    "        # Compute the gradient with PyTorch and compre with manual values\n",
    "        if verbose: print('Compute the gradient using PyTorch .backward()')\n",
    "        myloss.backward()\n",
    "        if verbose:\n",
    "            print_grad(w.grad.numpy(),b.grad.numpy())\n",
    "            print(\"\")\n",
    "        w.grad.zero_() # clear PyTorch's gradient\n",
    "        b.grad.zero_()\n",
    "        \n",
    "        # Parameter update with gradient descent\n",
    "        with torch.no_grad():\n",
    "            # TODO : YOUR PARAMETER UPDATE CODE GOES HERE\n",
    "            #  two lines of the form:\n",
    "            w -=   alpha * w_grad\n",
    "            b -=   alpha * b_grad\n",
    "            # raise Exception('Replace with your code.')\n",
    "            \n",
    "    if verbose==True: verbose=False\n",
    "    track_error.append(error_epoch)\n",
    "    if e % 50 == 0:\n",
    "        print(\"epoch \" + str(e) + \"; error=\" +str(round(error_epoch,3)))\n",
    "    \n",
    "# track output of gradient descent\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(track_error)\n",
    "plt.title('stochastic gradient descent (logistic activation)')\n",
    "plt.ylabel('error for epoch')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's change the activation function to \"tanh\" from the \"logistic\" function, such that $g(\\textbf{net}) = \\tanh(\\textbf{net})$. Here is an implementation of tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_tanh(x):\n",
    "    return (torch.exp(x) - torch.exp(-x))/(torch.exp(x) + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the tanh function is as follows:\n",
    "\n",
    "$$\\frac{\\partial g(\\textbf{net})}{\\partial \\textbf{net}}= \\frac{\\partial \\tanh(\\textbf{net})}{\\partial \\textbf{net}} = 1.0 - (\\tanh(\\textbf{net}))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 3 (5 points) </h3>\n",
    "<br>\n",
    "Just as before, fill in the missing code fragments for implementing gradient descent. This time we are using the tanh activation function. Be sure to change your gradient calculation to reflect the new activation function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute the gradient manually\n",
      " Input: [1. 0.]\n",
      " Output: 0.266\n",
      " Target: 1.0\n",
      "  d_loss / d_w = [-1.3644136 -0.       ]\n",
      "  d_loss / d_b = [-1.3644136]\n",
      "Compute the gradient using PyTorch .backward()\n",
      "  d_loss / d_w = [-1.3644136 -0.       ]\n",
      "  d_loss / d_b = [-1.3644136]\n",
      "\n",
      "Compute the gradient manually\n",
      " Input: [0. 0.]\n",
      " Output: 0.829\n",
      " Target: 0.0\n",
      "  d_loss / d_w = [0. 0.]\n",
      "  d_loss / d_b = [0.51883215]\n",
      "Compute the gradient using PyTorch .backward()\n",
      "  d_loss / d_w = [0. 0.]\n",
      "  d_loss / d_b = [0.518832]\n",
      "\n",
      "Compute the gradient manually\n",
      " Input: [0. 1.]\n",
      " Output: 0.654\n",
      " Target: 1.0\n",
      "  d_loss / d_w = [-0.         -0.39595485]\n",
      "  d_loss / d_b = [-0.39595485]\n",
      "Compute the gradient using PyTorch .backward()\n",
      "  d_loss / d_w = [ 0.         -0.39595485]\n",
      "  d_loss / d_b = [-0.39595485]\n",
      "\n",
      "Compute the gradient manually\n",
      " Input: [1. 1.]\n",
      " Output: 0.046\n",
      " Target: 1.0\n",
      "  d_loss / d_w = [-1.9034358 -1.9034358]\n",
      "  d_loss / d_b = [-1.9034358]\n",
      "Compute the gradient using PyTorch .backward()\n",
      "  d_loss / d_w = [-1.9034357 -1.9034357]\n",
      "  d_loss / d_b = [-1.9034357]\n",
      "\n",
      "epoch 0; error=2.255\n",
      "epoch 50; error=0.716\n",
      "epoch 100; error=0.231\n",
      "epoch 150; error=0.075\n",
      "epoch 200; error=0.038\n",
      "epoch 250; error=0.024\n",
      "epoch 300; error=0.018\n",
      "epoch 350; error=0.014\n",
      "epoch 400; error=0.011\n",
      "epoch 450; error=0.009\n",
      "epoch 500; error=0.008\n",
      "epoch 550; error=0.007\n",
      "epoch 600; error=0.006\n",
      "epoch 650; error=0.006\n",
      "epoch 700; error=0.005\n",
      "epoch 750; error=0.005\n",
      "epoch 800; error=0.004\n",
      "epoch 850; error=0.004\n",
      "epoch 900; error=0.004\n",
      "epoch 950; error=0.003\n",
      "epoch 1000; error=0.003\n",
      "epoch 1050; error=0.003\n",
      "epoch 1100; error=0.003\n",
      "epoch 1150; error=0.003\n",
      "epoch 1200; error=0.003\n",
      "epoch 1250; error=0.002\n",
      "epoch 1300; error=0.002\n",
      "epoch 1350; error=0.002\n",
      "epoch 1400; error=0.002\n",
      "epoch 1450; error=0.002\n",
      "epoch 1500; error=0.002\n",
      "epoch 1550; error=0.002\n",
      "epoch 1600; error=0.002\n",
      "epoch 1650; error=0.002\n",
      "epoch 1700; error=0.002\n",
      "epoch 1750; error=0.002\n",
      "epoch 1800; error=0.002\n",
      "epoch 1850; error=0.002\n",
      "epoch 1900; error=0.002\n",
      "epoch 1950; error=0.001\n",
      "epoch 2000; error=0.001\n",
      "epoch 2050; error=0.001\n",
      "epoch 2100; error=0.001\n",
      "epoch 2150; error=0.001\n",
      "epoch 2200; error=0.001\n",
      "epoch 2250; error=0.001\n",
      "epoch 2300; error=0.001\n",
      "epoch 2350; error=0.001\n",
      "epoch 2400; error=0.001\n",
      "epoch 2450; error=0.001\n",
      "epoch 2500; error=0.001\n",
      "epoch 2550; error=0.001\n",
      "epoch 2600; error=0.001\n",
      "epoch 2650; error=0.001\n",
      "epoch 2700; error=0.001\n",
      "epoch 2750; error=0.001\n",
      "epoch 2800; error=0.001\n",
      "epoch 2850; error=0.001\n",
      "epoch 2900; error=0.001\n",
      "epoch 2950; error=0.001\n",
      "epoch 3000; error=0.001\n",
      "epoch 3050; error=0.001\n",
      "epoch 3100; error=0.001\n",
      "epoch 3150; error=0.001\n",
      "epoch 3200; error=0.001\n",
      "epoch 3250; error=0.001\n",
      "epoch 3300; error=0.001\n",
      "epoch 3350; error=0.001\n",
      "epoch 3400; error=0.001\n",
      "epoch 3450; error=0.001\n",
      "epoch 3500; error=0.001\n",
      "epoch 3550; error=0.001\n",
      "epoch 3600; error=0.001\n",
      "epoch 3650; error=0.001\n",
      "epoch 3700; error=0.001\n",
      "epoch 3750; error=0.001\n",
      "epoch 3800; error=0.001\n",
      "epoch 3850; error=0.001\n",
      "epoch 3900; error=0.001\n",
      "epoch 3950; error=0.001\n",
      "epoch 4000; error=0.001\n",
      "epoch 4050; error=0.001\n",
      "epoch 4100; error=0.001\n",
      "epoch 4150; error=0.001\n",
      "epoch 4200; error=0.001\n",
      "epoch 4250; error=0.001\n",
      "epoch 4300; error=0.001\n",
      "epoch 4350; error=0.001\n",
      "epoch 4400; error=0.001\n",
      "epoch 4450; error=0.001\n",
      "epoch 4500; error=0.001\n",
      "epoch 4550; error=0.001\n",
      "epoch 4600; error=0.001\n",
      "epoch 4650; error=0.001\n",
      "epoch 4700; error=0.001\n",
      "epoch 4750; error=0.001\n",
      "epoch 4800; error=0.001\n",
      "epoch 4850; error=0.001\n",
      "epoch 4900; error=0.001\n",
      "epoch 4950; error=0.001\n",
      "Final result:\n",
      " Input: [0. 0.]\n",
      " Output: 0.001\n",
      " Target: 0.0\n",
      "\n",
      "Final result:\n",
      " Input: [1. 1.]\n",
      " Output: 1.0\n",
      " Target: 1.0\n",
      "\n",
      "Final result:\n",
      " Input: [0. 1.]\n",
      " Output: 0.984\n",
      " Target: 1.0\n",
      "\n",
      "Final result:\n",
      " Input: [1. 0.]\n",
      " Output: 0.984\n",
      " Target: 1.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "#     Although you will implement gradient descent manually, let's set requires_grad=True\n",
    "#     anyway so PyTorch will track the gradient too, and we can compare your gradient with PyTorch's.\n",
    "w = torch.randn(2) # [size 2] tensor\n",
    "b = torch.randn(1) # [size 1] tensor\n",
    "w.requires_grad = True\n",
    "b.requires_grad = True\n",
    "\n",
    "alpha = 0.05 # learning rate\n",
    "nepochs = 5000 # number of epochs\n",
    "\n",
    "track_error = []\n",
    "verbose = True\n",
    "for e in range(nepochs): # for each epoch\n",
    "    error_epoch = 0. # sum loss across the epoch\n",
    "    perm = np.random.permutation(N)\n",
    "    for p in perm: # visit data points in random order\n",
    "        x = D[p,:] # input pattern\n",
    "        \n",
    "        # compute output of neuron\n",
    "        net = torch.dot(x,w)+b\n",
    "        yhat = g_tanh(net)\n",
    "        \n",
    "        # compute loss\n",
    "        y = Y_or[p]\n",
    "        myloss = loss(yhat,y)\n",
    "        error_epoch += myloss.item()\n",
    "        \n",
    "        # print output if this is the last epoch\n",
    "        if (e == nepochs-1):\n",
    "            print(\"Final result:\")\n",
    "            print_forward(x,yhat,y)\n",
    "            print(\"\")\n",
    "\n",
    "        # Compute the gradient manually\n",
    "        if verbose:\n",
    "            print('Compute the gradient manually')\n",
    "            print_forward(x,yhat,y)\n",
    "        with torch.no_grad():\n",
    "            # TODO : YOUR GRADIENT CODE GOES HERE\n",
    "            #  two lines of the form\n",
    "            #    w_grad = ...    ([size 2] PyTorch tensor)\n",
    "            #    b_grad = ...    ([size 1] PyTorch tensor)\n",
    "            w_grad = 2 * (yhat - y) * (1 - g_tanh(net)**2) * x\n",
    "            b_grad = 2 * (yhat - y) * (1 - g_tanh(net)**2) * 1\n",
    "            #  make sure to inclose your code in the \"with torch.no_grad()\" wrapper,\n",
    "            #   otherwise PyTorch will try to track the \"gradient\" of the graident computation, which we don't want.\n",
    "            #raise Exception('Replace with your code.')                       \n",
    "        if verbose: print_grad(w_grad.numpy(),b_grad.numpy())\n",
    "\n",
    "        # Compute the gradient with PyTorch and compre with manual values\n",
    "        if verbose: print('Compute the gradient using PyTorch .backward()')\n",
    "        myloss.backward()\n",
    "        if verbose:\n",
    "            print_grad(w.grad.numpy(),b.grad.numpy())\n",
    "            print(\"\")\n",
    "        w.grad.zero_() # clear PyTorch's gradient\n",
    "        b.grad.zero_()\n",
    "        \n",
    "        # Parameter update with gradient descent\n",
    "        with torch.no_grad():\n",
    "            # TODO : YOUR PARAMETER UPDATE CODE GOES HERE\n",
    "            #  two lines of the form:\n",
    "            w -=   alpha * w_grad\n",
    "            b -=   alpha * b_grad\n",
    "            # raise Exception('Replace with your code.')\n",
    "            \n",
    "    if verbose==True: verbose=False\n",
    "    track_error.append(error_epoch)\n",
    "    if e % 50 == 0:\n",
    "        print(\"epoch \" + str(e) + \"; error=\" +str(round(error_epoch,3)))\n",
    "    \n",
    "# track output of gradient descent\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(track_error)\n",
    "plt.title('stochastic gradient descent (tanh activation)')\n",
    "plt.ylabel('error for epoch')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part, we have a simple multi-layer network with two input neurons, one hidden neuron, and one output neuron. Both the hidden and output unit should use the logistic activation function. We will learn to compute logical XOR. The network and logical XOR are shown below, for inputs $x_0$ and $x_1$ and target output $y$.\n",
    "\n",
    "<img src=\"images/nn_XOR.jpeg\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 4 (15 points) </h3>\n",
    "<br>\n",
    "You will implement backpropagation for this simple network. In the code below, you have several parts to fill in. First, define the forward pass to compute the output `yhat` from the input `x`. Second, fill in code to manually compute the gradients for all five weights w and two biases b in closed form. Third, fill in the code for updating the biases and weights.\n",
    "</div>\n",
    "\n",
    "After completing the code, run it to compare **your gradients** with the **ground-truth computed by PyTorch.** (There may be small differences that you shouldn't worry about, e.g. within 1e-6). Also, you can check the network's performance at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute the gradient manually\n",
      " Input: [0. 0.]\n",
      " Output: 0.8\n",
      " Target: 0.0\n",
      " Grad for w_34 and b_0\n",
      "  d_loss / d_w = [0. 0.]\n",
      "  d_loss / d_b = [0.05134995]\n",
      " Grad for w_012 and b_1\n",
      "  d_loss / d_w = [0.         0.         0.05730971]\n",
      "  d_loss / d_b = [0.25612253]\n",
      "\n",
      "Compute the gradient using PyTorch .backward()\n",
      " Grad for w_34 and b_0\n",
      "  d_loss / d_w = [0. 0.]\n",
      "  d_loss / d_b = [0.05134995]\n",
      " Grad for w_012 and b_1\n",
      "  d_loss / d_w = [0.         0.         0.05730971]\n",
      "  d_loss / d_b = [0.25612256]\n",
      "\n",
      "Compute the gradient manually\n",
      " Input: [1. 0.]\n",
      " Output: 0.941\n",
      " Target: 1.0\n",
      " Grad for w_34 and b_0\n",
      "  d_loss / d_w = [-0.00185046 -0.        ]\n",
      "  d_loss / d_b = [-0.00185046]\n",
      " Grad for w_012 and b_1\n",
      "  d_loss / d_w = [-0.00649488 -0.         -0.00357594]\n",
      "  d_loss / d_b = [-0.00649488]\n",
      "\n",
      "Compute the gradient using PyTorch .backward()\n",
      " Grad for w_34 and b_0\n",
      "  d_loss / d_w = [-0.00185046  0.        ]\n",
      "  d_loss / d_b = [-0.00185046]\n",
      " Grad for w_012 and b_1\n",
      "  d_loss / d_w = [-0.00649488  0.         -0.00357594]\n",
      "  d_loss / d_b = [-0.00649488]\n",
      "\n",
      "Compute the gradient manually\n",
      " Input: [1. 1.]\n",
      " Output: 0.869\n",
      " Target: 0.0\n",
      " Grad for w_34 and b_0\n",
      "  d_loss / d_w = [0.05431588 0.05431588]\n",
      "  d_loss / d_b = [0.05431588]\n",
      " Grad for w_012 and b_1\n",
      "  d_loss / d_w = [0.19828781 0.19828781 0.12098857]\n",
      "  d_loss / d_b = [0.19828781]\n",
      "\n",
      "Compute the gradient using PyTorch .backward()\n",
      " Grad for w_34 and b_0\n",
      "  d_loss / d_w = [0.05431589 0.05431589]\n",
      "  d_loss / d_b = [0.05431589]\n",
      " Grad for w_012 and b_1\n",
      "  d_loss / d_w = [0.19828786 0.19828786 0.12098859]\n",
      "  d_loss / d_b = [0.19828786]\n",
      "\n",
      "Compute the gradient manually\n",
      " Input: [0. 1.]\n",
      " Output: 0.61\n",
      " Target: 1.0\n",
      " Grad for w_34 and b_0\n",
      "  d_loss / d_w = [-0.         -0.04170353]\n",
      "  d_loss / d_b = [-0.04170353]\n",
      " Grad for w_012 and b_1\n",
      "  d_loss / d_w = [-0.         -0.18576019 -0.049704  ]\n",
      "  d_loss / d_b = [-0.18576019]\n",
      "\n",
      "Compute the gradient using PyTorch .backward()\n",
      " Grad for w_34 and b_0\n",
      "  d_loss / d_w = [ 0.         -0.04170353]\n",
      "  d_loss / d_b = [-0.04170353]\n",
      " Grad for w_012 and b_1\n",
      "  d_loss / d_w = [ 0.         -0.18576019 -0.049704  ]\n",
      "  d_loss / d_b = [-0.18576019]\n",
      "\n",
      "epoch 0; error=1.55\n",
      "w_34 tensor([1.4468, 0.2442], requires_grad=True)\n",
      "w_012 tensor([ 1.0166, -0.9556,  1.1480], requires_grad=True)\n",
      "b_0 tensor([-1.2470], requires_grad=True)\n",
      "b_1 tensor([1.1137], requires_grad=True)\n",
      "epoch 50; error=1.159\n",
      "w_34 tensor([1.3066, 0.1959], requires_grad=True)\n",
      "w_012 tensor([ 0.4290, -1.0102,  0.8124], requires_grad=True)\n",
      "b_0 tensor([-1.3925], requires_grad=True)\n",
      "b_1 tensor([0.4271], requires_grad=True)\n",
      "epoch 100; error=1.065\n",
      "w_34 tensor([1.2530, 0.1782], requires_grad=True)\n",
      "w_012 tensor([ 0.1449, -0.8788,  0.6769], requires_grad=True)\n",
      "b_0 tensor([-1.4354], requires_grad=True)\n",
      "b_1 tensor([0.1597], requires_grad=True)\n",
      "epoch 150; error=1.045\n",
      "w_34 tensor([1.2351, 0.1707], requires_grad=True)\n",
      "w_012 tensor([ 0.0438, -0.6911,  0.6354], requires_grad=True)\n",
      "b_0 tensor([-1.4416], requires_grad=True)\n",
      "b_1 tensor([0.0993], requires_grad=True)\n",
      "epoch 200; error=1.032\n",
      "w_34 tensor([1.2236, 0.1592], requires_grad=True)\n",
      "w_012 tensor([-0.0205, -0.5334,  0.6090], requires_grad=True)\n",
      "b_0 tensor([-1.4429], requires_grad=True)\n",
      "b_1 tensor([0.0710], requires_grad=True)\n",
      "epoch 250; error=1.025\n",
      "w_34 tensor([1.2150, 0.1431], requires_grad=True)\n",
      "w_012 tensor([-0.0666, -0.4098,  0.5879], requires_grad=True)\n",
      "b_0 tensor([-1.4434], requires_grad=True)\n",
      "b_1 tensor([0.0497], requires_grad=True)\n",
      "epoch 300; error=1.02\n",
      "w_34 tensor([1.2089, 0.1237], requires_grad=True)\n",
      "w_012 tensor([-0.0981, -0.3157,  0.5707], requires_grad=True)\n",
      "b_0 tensor([-1.4437], requires_grad=True)\n",
      "b_1 tensor([0.0307], requires_grad=True)\n",
      "epoch 350; error=1.018\n",
      "w_34 tensor([1.2041, 0.1015], requires_grad=True)\n",
      "w_012 tensor([-0.1208, -0.2447,  0.5562], requires_grad=True)\n",
      "b_0 tensor([-1.4444], requires_grad=True)\n",
      "b_1 tensor([0.0128], requires_grad=True)\n",
      "epoch 400; error=1.016\n",
      "w_34 tensor([1.2006, 0.0771], requires_grad=True)\n",
      "w_012 tensor([-0.1354, -0.1912,  0.5448], requires_grad=True)\n",
      "b_0 tensor([-1.4452], requires_grad=True)\n",
      "b_1 tensor([-0.0026], requires_grad=True)\n",
      "epoch 450; error=1.015\n",
      "w_34 tensor([1.1982, 0.0515], requires_grad=True)\n",
      "w_012 tensor([-0.1421, -0.1490,  0.5372], requires_grad=True)\n",
      "b_0 tensor([-1.4460], requires_grad=True)\n",
      "b_1 tensor([-0.0138], requires_grad=True)\n",
      "epoch 500; error=1.014\n",
      "w_34 tensor([1.1965, 0.0244], requires_grad=True)\n",
      "w_012 tensor([-0.1461, -0.1172,  0.5312], requires_grad=True)\n",
      "b_0 tensor([-1.4475], requires_grad=True)\n",
      "b_1 tensor([-0.0256], requires_grad=True)\n",
      "epoch 550; error=1.014\n",
      "w_34 tensor([ 1.1952, -0.0040], requires_grad=True)\n",
      "w_012 tensor([-0.1484, -0.0936,  0.5273], requires_grad=True)\n",
      "b_0 tensor([-1.4493], requires_grad=True)\n",
      "b_1 tensor([-0.0363], requires_grad=True)\n",
      "epoch 600; error=1.013\n",
      "w_34 tensor([ 1.1946, -0.0332], requires_grad=True)\n",
      "w_012 tensor([-0.1479, -0.0733,  0.5263], requires_grad=True)\n",
      "b_0 tensor([-1.4514], requires_grad=True)\n",
      "b_1 tensor([-0.0434], requires_grad=True)\n",
      "epoch 650; error=1.013\n",
      "w_34 tensor([ 1.1943, -0.0635], requires_grad=True)\n",
      "w_012 tensor([-0.1482, -0.0566,  0.5267], requires_grad=True)\n",
      "b_0 tensor([-1.4542], requires_grad=True)\n",
      "b_1 tensor([-0.0515], requires_grad=True)\n",
      "epoch 700; error=1.012\n",
      "w_34 tensor([ 1.1944, -0.0949], requires_grad=True)\n",
      "w_012 tensor([-0.1481, -0.0442,  0.5290], requires_grad=True)\n",
      "b_0 tensor([-1.4575], requires_grad=True)\n",
      "b_1 tensor([-0.0591], requires_grad=True)\n",
      "epoch 750; error=1.012\n",
      "w_34 tensor([ 1.1953, -0.1272], requires_grad=True)\n",
      "w_012 tensor([-0.1460, -0.0330,  0.5343], requires_grad=True)\n",
      "b_0 tensor([-1.4610], requires_grad=True)\n",
      "b_1 tensor([-0.0638], requires_grad=True)\n",
      "epoch 800; error=1.011\n",
      "w_34 tensor([ 1.1965, -0.1607], requires_grad=True)\n",
      "w_012 tensor([-0.1455, -0.0244,  0.5411], requires_grad=True)\n",
      "b_0 tensor([-1.4652], requires_grad=True)\n",
      "b_1 tensor([-0.0699], requires_grad=True)\n",
      "epoch 850; error=1.011\n",
      "w_34 tensor([ 1.1983, -0.1954], requires_grad=True)\n",
      "w_012 tensor([-0.1447, -0.0168,  0.5504], requires_grad=True)\n",
      "b_0 tensor([-1.4697], requires_grad=True)\n",
      "b_1 tensor([-0.0742], requires_grad=True)\n",
      "epoch 900; error=1.01\n",
      "w_34 tensor([ 1.2009, -0.2313], requires_grad=True)\n",
      "w_012 tensor([-0.1438, -0.0095,  0.5620], requires_grad=True)\n",
      "b_0 tensor([-1.4747], requires_grad=True)\n",
      "b_1 tensor([-0.0783], requires_grad=True)\n",
      "epoch 950; error=1.009\n",
      "w_34 tensor([ 1.2041, -0.2689], requires_grad=True)\n",
      "w_012 tensor([-0.1450, -0.0050,  0.5753], requires_grad=True)\n",
      "b_0 tensor([-1.4805], requires_grad=True)\n",
      "b_1 tensor([-0.0848], requires_grad=True)\n",
      "epoch 1000; error=1.009\n",
      "w_34 tensor([ 1.2081, -0.3081], requires_grad=True)\n",
      "w_012 tensor([-1.4757e-01, -3.9031e-04,  5.9106e-01], requires_grad=True)\n",
      "b_0 tensor([-1.4867], requires_grad=True)\n",
      "b_1 tensor([-0.0902], requires_grad=True)\n",
      "epoch 1050; error=1.008\n",
      "w_34 tensor([ 1.2135, -0.3486], requires_grad=True)\n",
      "w_012 tensor([-0.1479,  0.0058,  0.6106], requires_grad=True)\n",
      "b_0 tensor([-1.4929], requires_grad=True)\n",
      "b_1 tensor([-0.0925], requires_grad=True)\n",
      "epoch 1100; error=1.007\n",
      "w_34 tensor([ 1.2199, -0.3913], requires_grad=True)\n",
      "w_012 tensor([-0.1508,  0.0104,  0.6319], requires_grad=True)\n",
      "b_0 tensor([-1.5000], requires_grad=True)\n",
      "b_1 tensor([-0.0979], requires_grad=True)\n",
      "epoch 1150; error=1.006\n",
      "w_34 tensor([ 1.2278, -0.4357], requires_grad=True)\n",
      "w_012 tensor([-0.1541,  0.0170,  0.6561], requires_grad=True)\n",
      "b_0 tensor([-1.5075], requires_grad=True)\n",
      "b_1 tensor([-0.1028], requires_grad=True)\n",
      "epoch 1200; error=1.004\n",
      "w_34 tensor([ 1.2374, -0.4823], requires_grad=True)\n",
      "w_012 tensor([-0.1579,  0.0234,  0.6834], requires_grad=True)\n",
      "b_0 tensor([-1.5156], requires_grad=True)\n",
      "b_1 tensor([-0.1085], requires_grad=True)\n",
      "epoch 1250; error=1.003\n",
      "w_34 tensor([ 1.2491, -0.5311], requires_grad=True)\n",
      "w_012 tensor([-0.1612,  0.0308,  0.7147], requires_grad=True)\n",
      "b_0 tensor([-1.5237], requires_grad=True)\n",
      "b_1 tensor([-0.1125], requires_grad=True)\n",
      "epoch 1300; error=1.001\n",
      "w_34 tensor([ 1.2627, -0.5828], requires_grad=True)\n",
      "w_012 tensor([-0.1673,  0.0371,  0.7486], requires_grad=True)\n",
      "b_0 tensor([-1.5325], requires_grad=True)\n",
      "b_1 tensor([-0.1191], requires_grad=True)\n",
      "epoch 1350; error=0.999\n",
      "w_34 tensor([ 1.2788, -0.6372], requires_grad=True)\n",
      "w_012 tensor([-0.1736,  0.0431,  0.7863], requires_grad=True)\n",
      "b_0 tensor([-1.5418], requires_grad=True)\n",
      "b_1 tensor([-0.1271], requires_grad=True)\n",
      "epoch 1400; error=0.997\n",
      "w_34 tensor([ 1.2981, -0.6944], requires_grad=True)\n",
      "w_012 tensor([-0.1796,  0.0516,  0.8284], requires_grad=True)\n",
      "b_0 tensor([-1.5512], requires_grad=True)\n",
      "b_1 tensor([-0.1340], requires_grad=True)\n",
      "epoch 1450; error=0.994\n",
      "w_34 tensor([ 1.3207, -0.7549], requires_grad=True)\n",
      "w_012 tensor([-0.1865,  0.0609,  0.8747], requires_grad=True)\n",
      "b_0 tensor([-1.5609], requires_grad=True)\n",
      "b_1 tensor([-0.1420], requires_grad=True)\n",
      "epoch 1500; error=0.991\n",
      "w_34 tensor([ 1.3465, -0.8191], requires_grad=True)\n",
      "w_012 tensor([-0.1972,  0.0709,  0.9247], requires_grad=True)\n",
      "b_0 tensor([-1.5715], requires_grad=True)\n",
      "b_1 tensor([-0.1525], requires_grad=True)\n",
      "epoch 1550; error=0.988\n",
      "w_34 tensor([ 1.3766, -0.8871], requires_grad=True)\n",
      "w_012 tensor([-0.2083,  0.0814,  0.9801], requires_grad=True)\n",
      "b_0 tensor([-1.5821], requires_grad=True)\n",
      "b_1 tensor([-0.1630], requires_grad=True)\n",
      "epoch 1600; error=0.983\n",
      "w_34 tensor([ 1.4113, -0.9591], requires_grad=True)\n",
      "w_012 tensor([-0.2200,  0.0940,  1.0410], requires_grad=True)\n",
      "b_0 tensor([-1.5927], requires_grad=True)\n",
      "b_1 tensor([-0.1734], requires_grad=True)\n",
      "epoch 1650; error=0.978\n",
      "w_34 tensor([ 1.4510, -1.0357], requires_grad=True)\n",
      "w_012 tensor([-0.2342,  0.1091,  1.1075], requires_grad=True)\n",
      "b_0 tensor([-1.6037], requires_grad=True)\n",
      "b_1 tensor([-0.1838], requires_grad=True)\n",
      "epoch 1700; error=0.973\n",
      "w_34 tensor([ 1.4965, -1.1168], requires_grad=True)\n",
      "w_012 tensor([-0.2489,  0.1258,  1.1801], requires_grad=True)\n",
      "b_0 tensor([-1.6151], requires_grad=True)\n",
      "b_1 tensor([-0.1968], requires_grad=True)\n",
      "epoch 1750; error=0.966\n",
      "w_34 tensor([ 1.5474, -1.2034], requires_grad=True)\n",
      "w_012 tensor([-0.2679,  0.1432,  1.2588], requires_grad=True)\n",
      "b_0 tensor([-1.6273], requires_grad=True)\n",
      "b_1 tensor([-0.2126], requires_grad=True)\n",
      "epoch 1800; error=0.957\n",
      "w_34 tensor([ 1.6043, -1.2955], requires_grad=True)\n",
      "w_012 tensor([-0.2906,  0.1634,  1.3440], requires_grad=True)\n",
      "b_0 tensor([-1.6406], requires_grad=True)\n",
      "b_1 tensor([-0.2307], requires_grad=True)\n",
      "epoch 1850; error=0.948\n",
      "w_34 tensor([ 1.6681, -1.3930], requires_grad=True)\n",
      "w_012 tensor([-0.3145,  0.1880,  1.4370], requires_grad=True)\n",
      "b_0 tensor([-1.6543], requires_grad=True)\n",
      "b_1 tensor([-0.2484], requires_grad=True)\n",
      "epoch 1900; error=0.936\n",
      "w_34 tensor([ 1.7386, -1.4967], requires_grad=True)\n",
      "w_012 tensor([-0.3425,  0.2140,  1.5376], requires_grad=True)\n",
      "b_0 tensor([-1.6693], requires_grad=True)\n",
      "b_1 tensor([-0.2694], requires_grad=True)\n",
      "epoch 1950; error=0.924\n",
      "w_34 tensor([ 1.8156, -1.6064], requires_grad=True)\n",
      "w_012 tensor([-0.3749,  0.2455,  1.6458], requires_grad=True)\n",
      "b_0 tensor([-1.6862], requires_grad=True)\n",
      "b_1 tensor([-0.2915], requires_grad=True)\n",
      "epoch 2000; error=0.908\n",
      "w_34 tensor([ 1.8989, -1.7222], requires_grad=True)\n",
      "w_012 tensor([-0.4121,  0.2798,  1.7617], requires_grad=True)\n",
      "b_0 tensor([-1.7057], requires_grad=True)\n",
      "b_1 tensor([-0.3185], requires_grad=True)\n",
      "epoch 2050; error=0.891\n",
      "w_34 tensor([ 1.9899, -1.8432], requires_grad=True)\n",
      "w_012 tensor([-0.4507,  0.3204,  1.8870], requires_grad=True)\n",
      "b_0 tensor([-1.7262], requires_grad=True)\n",
      "b_1 tensor([-0.3440], requires_grad=True)\n",
      "epoch 2100; error=0.872\n",
      "w_34 tensor([ 2.0860, -1.9701], requires_grad=True)\n",
      "w_012 tensor([-0.4961,  0.3632,  2.0196], requires_grad=True)\n",
      "b_0 tensor([-1.7505], requires_grad=True)\n",
      "b_1 tensor([-0.3755], requires_grad=True)\n",
      "epoch 2150; error=0.85\n",
      "w_34 tensor([ 2.1878, -2.1015], requires_grad=True)\n",
      "w_012 tensor([-0.5452,  0.4129,  2.1601], requires_grad=True)\n",
      "b_0 tensor([-1.7777], requires_grad=True)\n",
      "b_1 tensor([-0.4078], requires_grad=True)\n",
      "epoch 2200; error=0.826\n",
      "w_34 tensor([ 2.2931, -2.2369], requires_grad=True)\n",
      "w_012 tensor([-0.6011,  0.4678,  2.3068], requires_grad=True)\n",
      "b_0 tensor([-1.8094], requires_grad=True)\n",
      "b_1 tensor([-0.4448], requires_grad=True)\n",
      "epoch 2250; error=0.8\n",
      "w_34 tensor([ 2.4030, -2.3752], requires_grad=True)\n",
      "w_012 tensor([-0.6605,  0.5275,  2.4609], requires_grad=True)\n",
      "b_0 tensor([-1.8435], requires_grad=True)\n",
      "b_1 tensor([-0.4825], requires_grad=True)\n",
      "epoch 2300; error=0.772\n",
      "w_34 tensor([ 2.5165, -2.5149], requires_grad=True)\n",
      "w_012 tensor([-0.7229,  0.5938,  2.6211], requires_grad=True)\n",
      "b_0 tensor([-1.8801], requires_grad=True)\n",
      "b_1 tensor([-0.5204], requires_grad=True)\n",
      "epoch 2350; error=0.743\n",
      "w_34 tensor([ 2.6313, -2.6554], requires_grad=True)\n",
      "w_012 tensor([-0.7907,  0.6645,  2.7855], requires_grad=True)\n",
      "b_0 tensor([-1.9203], requires_grad=True)\n",
      "b_1 tensor([-0.5617], requires_grad=True)\n",
      "epoch 2400; error=0.712\n",
      "w_34 tensor([ 2.7473, -2.7952], requires_grad=True)\n",
      "w_012 tensor([-0.8620,  0.7407,  2.9535], requires_grad=True)\n",
      "b_0 tensor([-1.9629], requires_grad=True)\n",
      "b_1 tensor([-0.6035], requires_grad=True)\n",
      "epoch 2450; error=0.681\n",
      "w_34 tensor([ 2.8630, -2.9339], requires_grad=True)\n",
      "w_012 tensor([-0.9376,  0.8188,  3.1239], requires_grad=True)\n",
      "b_0 tensor([-2.0078], requires_grad=True)\n",
      "b_1 tensor([-0.6488], requires_grad=True)\n",
      "epoch 2500; error=0.649\n",
      "w_34 tensor([ 2.9795, -3.0696], requires_grad=True)\n",
      "w_012 tensor([-1.0136,  0.9020,  3.2969], requires_grad=True)\n",
      "b_0 tensor([-2.0525], requires_grad=True)\n",
      "b_1 tensor([-0.6912], requires_grad=True)\n",
      "epoch 2550; error=0.618\n",
      "w_34 tensor([ 3.0939, -3.2022], requires_grad=True)\n",
      "w_012 tensor([-1.0925,  0.9872,  3.4700], requires_grad=True)\n",
      "b_0 tensor([-2.0990], requires_grad=True)\n",
      "b_1 tensor([-0.7357], requires_grad=True)\n",
      "epoch 2600; error=0.587\n",
      "w_34 tensor([ 3.2067, -3.3309], requires_grad=True)\n",
      "w_012 tensor([-1.1727,  1.0734,  3.6429], requires_grad=True)\n",
      "b_0 tensor([-2.1455], requires_grad=True)\n",
      "b_1 tensor([-0.7810], requires_grad=True)\n",
      "epoch 2650; error=0.556\n",
      "w_34 tensor([ 3.3164, -3.4555], requires_grad=True)\n",
      "w_012 tensor([-1.2545,  1.1611,  3.8145], requires_grad=True)\n",
      "b_0 tensor([-2.1920], requires_grad=True)\n",
      "b_1 tensor([-0.8250], requires_grad=True)\n",
      "epoch 2700; error=0.527\n",
      "w_34 tensor([ 3.4234, -3.5757], requires_grad=True)\n",
      "w_012 tensor([-1.3366,  1.2484,  3.9845], requires_grad=True)\n",
      "b_0 tensor([-2.2377], requires_grad=True)\n",
      "b_1 tensor([-0.8687], requires_grad=True)\n",
      "epoch 2750; error=0.498\n",
      "w_34 tensor([ 3.5268, -3.6909], requires_grad=True)\n",
      "w_012 tensor([-1.4190,  1.3351,  4.1514], requires_grad=True)\n",
      "b_0 tensor([-2.2835], requires_grad=True)\n",
      "b_1 tensor([-0.9137], requires_grad=True)\n",
      "epoch 2800; error=0.471\n",
      "w_34 tensor([ 3.6265, -3.8012], requires_grad=True)\n",
      "w_012 tensor([-1.5008,  1.4221,  4.3153], requires_grad=True)\n",
      "b_0 tensor([-2.3281], requires_grad=True)\n",
      "b_1 tensor([-0.9570], requires_grad=True)\n",
      "epoch 2850; error=0.446\n",
      "w_34 tensor([ 3.7231, -3.9066], requires_grad=True)\n",
      "w_012 tensor([-1.5810,  1.5078,  4.4763], requires_grad=True)\n",
      "b_0 tensor([-2.3707], requires_grad=True)\n",
      "b_1 tensor([-0.9983], requires_grad=True)\n",
      "epoch 2900; error=0.421\n",
      "w_34 tensor([ 3.8158, -4.0071], requires_grad=True)\n",
      "w_012 tensor([-1.6602,  1.5922,  4.6335], requires_grad=True)\n",
      "b_0 tensor([-2.4121], requires_grad=True)\n",
      "b_1 tensor([-1.0387], requires_grad=True)\n",
      "epoch 2950; error=0.398\n",
      "w_34 tensor([ 3.9044, -4.1029], requires_grad=True)\n",
      "w_012 tensor([-1.7385,  1.6745,  4.7862], requires_grad=True)\n",
      "b_0 tensor([-2.4527], requires_grad=True)\n",
      "b_1 tensor([-1.0794], requires_grad=True)\n",
      "epoch 3000; error=0.377\n",
      "w_34 tensor([ 3.9897, -4.1941], requires_grad=True)\n",
      "w_012 tensor([-1.8147,  1.7555,  4.9352], requires_grad=True)\n",
      "b_0 tensor([-2.4912], requires_grad=True)\n",
      "b_1 tensor([-1.1175], requires_grad=True)\n",
      "epoch 3050; error=0.357\n",
      "w_34 tensor([ 4.0708, -4.2809], requires_grad=True)\n",
      "w_012 tensor([-1.8895,  1.8346,  5.0796], requires_grad=True)\n",
      "b_0 tensor([-2.5288], requires_grad=True)\n",
      "b_1 tensor([-1.1552], requires_grad=True)\n",
      "epoch 3100; error=0.338\n",
      "w_34 tensor([ 4.1487, -4.3636], requires_grad=True)\n",
      "w_012 tensor([-1.9626,  1.9107,  5.2201], requires_grad=True)\n",
      "b_0 tensor([-2.5644], requires_grad=True)\n",
      "b_1 tensor([-1.1915], requires_grad=True)\n",
      "epoch 3150; error=0.321\n",
      "w_34 tensor([ 4.2228, -4.4424], requires_grad=True)\n",
      "w_012 tensor([-2.0342,  1.9847,  5.3560], requires_grad=True)\n",
      "b_0 tensor([-2.5991], requires_grad=True)\n",
      "b_1 tensor([-1.2276], requires_grad=True)\n",
      "epoch 3200; error=0.305\n",
      "w_34 tensor([ 4.2941, -4.5173], requires_grad=True)\n",
      "w_012 tensor([-2.1029,  2.0569,  5.4882], requires_grad=True)\n",
      "b_0 tensor([-2.6317], requires_grad=True)\n",
      "b_1 tensor([-1.2613], requires_grad=True)\n",
      "epoch 3250; error=0.289\n",
      "w_34 tensor([ 4.3617, -4.5887], requires_grad=True)\n",
      "w_012 tensor([-2.1705,  2.1267,  5.6159], requires_grad=True)\n",
      "b_0 tensor([-2.6637], requires_grad=True)\n",
      "b_1 tensor([-1.2951], requires_grad=True)\n",
      "epoch 3300; error=0.275\n",
      "w_34 tensor([ 4.4268, -4.6568], requires_grad=True)\n",
      "w_012 tensor([-2.2359,  2.1936,  5.7399], requires_grad=True)\n",
      "b_0 tensor([-2.6939], requires_grad=True)\n",
      "b_1 tensor([-1.3276], requires_grad=True)\n",
      "epoch 3350; error=0.262\n",
      "w_34 tensor([ 4.4889, -4.7217], requires_grad=True)\n",
      "w_012 tensor([-2.2992,  2.2593,  5.8599], requires_grad=True)\n",
      "b_0 tensor([-2.7230], requires_grad=True)\n",
      "b_1 tensor([-1.3589], requires_grad=True)\n",
      "epoch 3400; error=0.25\n",
      "w_34 tensor([ 4.5481, -4.7836], requires_grad=True)\n",
      "w_012 tensor([-2.3609,  2.3227,  5.9760], requires_grad=True)\n",
      "b_0 tensor([-2.7510], requires_grad=True)\n",
      "b_1 tensor([-1.3890], requires_grad=True)\n",
      "epoch 3450; error=0.239\n",
      "w_34 tensor([ 4.6049, -4.8428], requires_grad=True)\n",
      "w_012 tensor([-2.4209,  2.3835,  6.0886], requires_grad=True)\n",
      "b_0 tensor([-2.7779], requires_grad=True)\n",
      "b_1 tensor([-1.4189], requires_grad=True)\n",
      "epoch 3500; error=0.228\n",
      "w_34 tensor([ 4.6595, -4.8993], requires_grad=True)\n",
      "w_012 tensor([-2.4786,  2.4434,  6.1977], requires_grad=True)\n",
      "b_0 tensor([-2.8034], requires_grad=True)\n",
      "b_1 tensor([-1.4465], requires_grad=True)\n",
      "epoch 3550; error=0.218\n",
      "w_34 tensor([ 4.7116, -4.9534], requires_grad=True)\n",
      "w_012 tensor([-2.5348,  2.5010,  6.3034], requires_grad=True)\n",
      "b_0 tensor([-2.8281], requires_grad=True)\n",
      "b_1 tensor([-1.4738], requires_grad=True)\n",
      "epoch 3600; error=0.209\n",
      "w_34 tensor([ 4.7615, -5.0052], requires_grad=True)\n",
      "w_012 tensor([-2.5897,  2.5564,  6.4056], requires_grad=True)\n",
      "b_0 tensor([-2.8520], requires_grad=True)\n",
      "b_1 tensor([-1.5011], requires_grad=True)\n",
      "epoch 3650; error=0.2\n",
      "w_34 tensor([ 4.8097, -5.0547], requires_grad=True)\n",
      "w_012 tensor([-2.6423,  2.6110,  6.5051], requires_grad=True)\n",
      "b_0 tensor([-2.8745], requires_grad=True)\n",
      "b_1 tensor([-1.5259], requires_grad=True)\n",
      "epoch 3700; error=0.192\n",
      "w_34 tensor([ 4.8558, -5.1022], requires_grad=True)\n",
      "w_012 tensor([-2.6935,  2.6633,  6.6015], requires_grad=True)\n",
      "b_0 tensor([-2.8963], requires_grad=True)\n",
      "b_1 tensor([-1.5508], requires_grad=True)\n",
      "epoch 3750; error=0.184\n",
      "w_34 tensor([ 4.9001, -5.1478], requires_grad=True)\n",
      "w_012 tensor([-2.7435,  2.7139,  6.6949], requires_grad=True)\n",
      "b_0 tensor([-2.9175], requires_grad=True)\n",
      "b_1 tensor([-1.5755], requires_grad=True)\n",
      "epoch 3800; error=0.177\n",
      "w_34 tensor([ 4.9429, -5.1916], requires_grad=True)\n",
      "w_012 tensor([-2.7917,  2.7632,  6.7858], requires_grad=True)\n",
      "b_0 tensor([-2.9376], requires_grad=True)\n",
      "b_1 tensor([-1.5986], requires_grad=True)\n",
      "epoch 3850; error=0.17\n",
      "w_34 tensor([ 4.9838, -5.2338], requires_grad=True)\n",
      "w_012 tensor([-2.8390,  2.8109,  6.8739], requires_grad=True)\n",
      "b_0 tensor([-2.9572], requires_grad=True)\n",
      "b_1 tensor([-1.6215], requires_grad=True)\n",
      "epoch 3900; error=0.164\n",
      "w_34 tensor([ 5.0233, -5.2744], requires_grad=True)\n",
      "w_012 tensor([-2.8847,  2.8573,  6.9596], requires_grad=True)\n",
      "b_0 tensor([-2.9760], requires_grad=True)\n",
      "b_1 tensor([-1.6434], requires_grad=True)\n",
      "epoch 3950; error=0.158\n",
      "w_34 tensor([ 5.0614, -5.3134], requires_grad=True)\n",
      "w_012 tensor([-2.9292,  2.9025,  7.0428], requires_grad=True)\n",
      "b_0 tensor([-2.9941], requires_grad=True)\n",
      "b_1 tensor([-1.6646], requires_grad=True)\n",
      "epoch 4000; error=0.153\n",
      "w_34 tensor([ 5.0982, -5.3510], requires_grad=True)\n",
      "w_012 tensor([-2.9723,  2.9463,  7.1239], requires_grad=True)\n",
      "b_0 tensor([-3.0116], requires_grad=True)\n",
      "b_1 tensor([-1.6853], requires_grad=True)\n",
      "epoch 4050; error=0.147\n",
      "w_34 tensor([ 5.1337, -5.3873], requires_grad=True)\n",
      "w_012 tensor([-3.0144,  2.9890,  7.2026], requires_grad=True)\n",
      "b_0 tensor([-3.0286], requires_grad=True)\n",
      "b_1 tensor([-1.7054], requires_grad=True)\n",
      "epoch 4100; error=0.142\n",
      "w_34 tensor([ 5.1679, -5.4223], requires_grad=True)\n",
      "w_012 tensor([-3.0553,  3.0304,  7.2793], requires_grad=True)\n",
      "b_0 tensor([-3.0450], requires_grad=True)\n",
      "b_1 tensor([-1.7252], requires_grad=True)\n",
      "epoch 4150; error=0.138\n",
      "w_34 tensor([ 5.2010, -5.4562], requires_grad=True)\n",
      "w_012 tensor([-3.0953,  3.0704,  7.3539], requires_grad=True)\n",
      "b_0 tensor([-3.0609], requires_grad=True)\n",
      "b_1 tensor([-1.7446], requires_grad=True)\n",
      "epoch 4200; error=0.133\n",
      "w_34 tensor([ 5.2330, -5.4889], requires_grad=True)\n",
      "w_012 tensor([-3.1342,  3.1095,  7.4266], requires_grad=True)\n",
      "b_0 tensor([-3.0763], requires_grad=True)\n",
      "b_1 tensor([-1.7634], requires_grad=True)\n",
      "epoch 4250; error=0.129\n",
      "w_34 tensor([ 5.2639, -5.5205], requires_grad=True)\n",
      "w_012 tensor([-3.1721,  3.1477,  7.4975], requires_grad=True)\n",
      "b_0 tensor([-3.0912], requires_grad=True)\n",
      "b_1 tensor([-1.7816], requires_grad=True)\n",
      "epoch 4300; error=0.125\n",
      "w_34 tensor([ 5.2941, -5.5511], requires_grad=True)\n",
      "w_012 tensor([-3.2088,  3.1853,  7.5666], requires_grad=True)\n",
      "b_0 tensor([-3.1055], requires_grad=True)\n",
      "b_1 tensor([-1.7988], requires_grad=True)\n",
      "epoch 4350; error=0.121\n",
      "w_34 tensor([ 5.3232, -5.5807], requires_grad=True)\n",
      "w_012 tensor([-3.2447,  3.2215,  7.6341], requires_grad=True)\n",
      "b_0 tensor([-3.1195], requires_grad=True)\n",
      "b_1 tensor([-1.8162], requires_grad=True)\n",
      "epoch 4400; error=0.118\n",
      "w_34 tensor([ 5.3515, -5.6094], requires_grad=True)\n",
      "w_012 tensor([-3.2796,  3.2572,  7.6999], requires_grad=True)\n",
      "b_0 tensor([-3.1330], requires_grad=True)\n",
      "b_1 tensor([-1.8327], requires_grad=True)\n",
      "epoch 4450; error=0.114\n",
      "w_34 tensor([ 5.3788, -5.6372], requires_grad=True)\n",
      "w_012 tensor([-3.3140,  3.2914,  7.7641], requires_grad=True)\n",
      "b_0 tensor([-3.1462], requires_grad=True)\n",
      "b_1 tensor([-1.8494], requires_grad=True)\n",
      "epoch 4500; error=0.111\n",
      "w_34 tensor([ 5.4054, -5.6643], requires_grad=True)\n",
      "w_012 tensor([-3.3476,  3.3249,  7.8268], requires_grad=True)\n",
      "b_0 tensor([-3.1590], requires_grad=True)\n",
      "b_1 tensor([-1.8657], requires_grad=True)\n",
      "epoch 4550; error=0.108\n",
      "w_34 tensor([ 5.4313, -5.6905], requires_grad=True)\n",
      "w_012 tensor([-3.3801,  3.3581,  7.8882], requires_grad=True)\n",
      "b_0 tensor([-3.1713], requires_grad=True)\n",
      "b_1 tensor([-1.8810], requires_grad=True)\n",
      "epoch 4600; error=0.105\n",
      "w_34 tensor([ 5.4564, -5.7160], requires_grad=True)\n",
      "w_012 tensor([-3.4121,  3.3900,  7.9481], requires_grad=True)\n",
      "b_0 tensor([-3.1834], requires_grad=True)\n",
      "b_1 tensor([-1.8966], requires_grad=True)\n",
      "epoch 4650; error=0.102\n",
      "w_34 tensor([ 5.4807, -5.7408], requires_grad=True)\n",
      "w_012 tensor([-3.4433,  3.4214,  8.0067], requires_grad=True)\n",
      "b_0 tensor([-3.1952], requires_grad=True)\n",
      "b_1 tensor([-1.9115], requires_grad=True)\n",
      "epoch 4700; error=0.1\n",
      "w_34 tensor([ 5.5045, -5.7648], requires_grad=True)\n",
      "w_012 tensor([-3.4738,  3.4521,  8.0640], requires_grad=True)\n",
      "b_0 tensor([-3.2066], requires_grad=True)\n",
      "b_1 tensor([-1.9261], requires_grad=True)\n",
      "epoch 4750; error=0.097\n",
      "w_34 tensor([ 5.5276, -5.7883], requires_grad=True)\n",
      "w_012 tensor([-3.5036,  3.4822,  8.1201], requires_grad=True)\n",
      "b_0 tensor([-3.2177], requires_grad=True)\n",
      "b_1 tensor([-1.9403], requires_grad=True)\n",
      "epoch 4800; error=0.095\n",
      "w_34 tensor([ 5.5501, -5.8111], requires_grad=True)\n",
      "w_012 tensor([-3.5329,  3.5116,  8.1749], requires_grad=True)\n",
      "b_0 tensor([-3.2286], requires_grad=True)\n",
      "b_1 tensor([-1.9544], requires_grad=True)\n",
      "epoch 4850; error=0.092\n",
      "w_34 tensor([ 5.5721, -5.8333], requires_grad=True)\n",
      "w_012 tensor([-3.5614,  3.5404,  8.2286], requires_grad=True)\n",
      "b_0 tensor([-3.2392], requires_grad=True)\n",
      "b_1 tensor([-1.9680], requires_grad=True)\n",
      "epoch 4900; error=0.09\n",
      "w_34 tensor([ 5.5934, -5.8549], requires_grad=True)\n",
      "w_012 tensor([-3.5895,  3.5685,  8.2812], requires_grad=True)\n",
      "b_0 tensor([-3.2495], requires_grad=True)\n",
      "b_1 tensor([-1.9815], requires_grad=True)\n",
      "epoch 4950; error=0.088\n",
      "w_34 tensor([ 5.6143, -5.8760], requires_grad=True)\n",
      "w_012 tensor([-3.6169,  3.5961,  8.3328], requires_grad=True)\n",
      "b_0 tensor([-3.2596], requires_grad=True)\n",
      "b_1 tensor([-1.9946], requires_grad=True)\n",
      "epoch 5000; error=0.086\n",
      "w_34 tensor([ 5.6347, -5.8966], requires_grad=True)\n",
      "w_012 tensor([-3.6437,  3.6231,  8.3834], requires_grad=True)\n",
      "b_0 tensor([-3.2693], requires_grad=True)\n",
      "b_1 tensor([-2.0073], requires_grad=True)\n",
      "epoch 5050; error=0.084\n",
      "w_34 tensor([ 5.6545, -5.9167], requires_grad=True)\n",
      "w_012 tensor([-3.6701,  3.6495,  8.4329], requires_grad=True)\n",
      "b_0 tensor([-3.2790], requires_grad=True)\n",
      "b_1 tensor([-2.0201], requires_grad=True)\n",
      "epoch 5100; error=0.082\n",
      "w_34 tensor([ 5.6738, -5.9363], requires_grad=True)\n",
      "w_012 tensor([-3.6960,  3.6755,  8.4815], requires_grad=True)\n",
      "b_0 tensor([-3.2884], requires_grad=True)\n",
      "b_1 tensor([-2.0325], requires_grad=True)\n",
      "epoch 5150; error=0.08\n",
      "w_34 tensor([ 5.6928, -5.9554], requires_grad=True)\n",
      "w_012 tensor([-3.7212,  3.7011,  8.5292], requires_grad=True)\n",
      "b_0 tensor([-3.2975], requires_grad=True)\n",
      "b_1 tensor([-2.0443], requires_grad=True)\n",
      "epoch 5200; error=0.079\n",
      "w_34 tensor([ 5.7113, -5.9741], requires_grad=True)\n",
      "w_012 tensor([-3.7461,  3.7261,  8.5760], requires_grad=True)\n",
      "b_0 tensor([-3.3064], requires_grad=True)\n",
      "b_1 tensor([-2.0562], requires_grad=True)\n",
      "epoch 5250; error=0.077\n",
      "w_34 tensor([ 5.7294, -5.9924], requires_grad=True)\n",
      "w_012 tensor([-3.7705,  3.7505,  8.6219], requires_grad=True)\n",
      "b_0 tensor([-3.3152], requires_grad=True)\n",
      "b_1 tensor([-2.0680], requires_grad=True)\n",
      "epoch 5300; error=0.075\n",
      "w_34 tensor([ 5.7471, -6.0102], requires_grad=True)\n",
      "w_012 tensor([-3.7945,  3.7744,  8.6670], requires_grad=True)\n",
      "b_0 tensor([-3.3237], requires_grad=True)\n",
      "b_1 tensor([-2.0795], requires_grad=True)\n",
      "epoch 5350; error=0.074\n",
      "w_34 tensor([ 5.7643, -6.0277], requires_grad=True)\n",
      "w_012 tensor([-3.8181,  3.7978,  8.7113], requires_grad=True)\n",
      "b_0 tensor([-3.3321], requires_grad=True)\n",
      "b_1 tensor([-2.0910], requires_grad=True)\n",
      "epoch 5400; error=0.072\n",
      "w_34 tensor([ 5.7813, -6.0448], requires_grad=True)\n",
      "w_012 tensor([-3.8411,  3.8211,  8.7548], requires_grad=True)\n",
      "b_0 tensor([-3.3403], requires_grad=True)\n",
      "b_1 tensor([-2.1019], requires_grad=True)\n",
      "epoch 5450; error=0.071\n",
      "w_34 tensor([ 5.7978, -6.0615], requires_grad=True)\n",
      "w_012 tensor([-3.8638,  3.8439,  8.7975], requires_grad=True)\n",
      "b_0 tensor([-3.3483], requires_grad=True)\n",
      "b_1 tensor([-2.1127], requires_grad=True)\n",
      "epoch 5500; error=0.069\n",
      "w_34 tensor([ 5.8141, -6.0779], requires_grad=True)\n",
      "w_012 tensor([-3.8860,  3.8662,  8.8396], requires_grad=True)\n",
      "b_0 tensor([-3.3562], requires_grad=True)\n",
      "b_1 tensor([-2.1233], requires_grad=True)\n",
      "epoch 5550; error=0.068\n",
      "w_34 tensor([ 5.8300, -6.0939], requires_grad=True)\n",
      "w_012 tensor([-3.9079,  3.8881,  8.8809], requires_grad=True)\n",
      "b_0 tensor([-3.3639], requires_grad=True)\n",
      "b_1 tensor([-2.1338], requires_grad=True)\n",
      "epoch 5600; error=0.067\n",
      "w_34 tensor([ 5.8456, -6.1096], requires_grad=True)\n",
      "w_012 tensor([-3.9294,  3.9098,  8.9216], requires_grad=True)\n",
      "b_0 tensor([-3.3714], requires_grad=True)\n",
      "b_1 tensor([-2.1441], requires_grad=True)\n",
      "epoch 5650; error=0.066\n",
      "w_34 tensor([ 5.8608, -6.1250], requires_grad=True)\n",
      "w_012 tensor([-3.9506,  3.9310,  8.9615], requires_grad=True)\n",
      "b_0 tensor([-3.3788], requires_grad=True)\n",
      "b_1 tensor([-2.1543], requires_grad=True)\n",
      "epoch 5700; error=0.064\n",
      "w_34 tensor([ 5.8758, -6.1401], requires_grad=True)\n",
      "w_012 tensor([-3.9714,  3.9519,  9.0008], requires_grad=True)\n",
      "b_0 tensor([-3.3861], requires_grad=True)\n",
      "b_1 tensor([-2.1641], requires_grad=True)\n",
      "epoch 5750; error=0.063\n",
      "w_34 tensor([ 5.8905, -6.1549], requires_grad=True)\n",
      "w_012 tensor([-3.9919,  3.9725,  9.0395], requires_grad=True)\n",
      "b_0 tensor([-3.3932], requires_grad=True)\n",
      "b_1 tensor([-2.1740], requires_grad=True)\n",
      "epoch 5800; error=0.062\n",
      "w_34 tensor([ 5.9049, -6.1694], requires_grad=True)\n",
      "w_012 tensor([-4.0121,  3.9927,  9.0775], requires_grad=True)\n",
      "b_0 tensor([-3.4002], requires_grad=True)\n",
      "b_1 tensor([-2.1836], requires_grad=True)\n",
      "epoch 5850; error=0.061\n",
      "w_34 tensor([ 5.9190, -6.1837], requires_grad=True)\n",
      "w_012 tensor([-4.0320,  4.0125,  9.1150], requires_grad=True)\n",
      "b_0 tensor([-3.4071], requires_grad=True)\n",
      "b_1 tensor([-2.1932], requires_grad=True)\n",
      "epoch 5900; error=0.06\n",
      "w_34 tensor([ 5.9328, -6.1976], requires_grad=True)\n",
      "w_012 tensor([-4.0516,  4.0320,  9.1519], requires_grad=True)\n",
      "b_0 tensor([-3.4138], requires_grad=True)\n",
      "b_1 tensor([-2.2026], requires_grad=True)\n",
      "epoch 5950; error=0.059\n",
      "w_34 tensor([ 5.9464, -6.2113], requires_grad=True)\n",
      "w_012 tensor([-4.0708,  4.0514,  9.1882], requires_grad=True)\n",
      "b_0 tensor([-3.4204], requires_grad=True)\n",
      "b_1 tensor([-2.2117], requires_grad=True)\n",
      "epoch 6000; error=0.058\n",
      "w_34 tensor([ 5.9598, -6.2248], requires_grad=True)\n",
      "w_012 tensor([-4.0898,  4.0703,  9.2240], requires_grad=True)\n",
      "b_0 tensor([-3.4269], requires_grad=True)\n",
      "b_1 tensor([-2.2209], requires_grad=True)\n",
      "epoch 6050; error=0.057\n",
      "w_34 tensor([ 5.9729, -6.2380], requires_grad=True)\n",
      "w_012 tensor([-4.1084,  4.0890,  9.2592], requires_grad=True)\n",
      "b_0 tensor([-3.4333], requires_grad=True)\n",
      "b_1 tensor([-2.2298], requires_grad=True)\n",
      "epoch 6100; error=0.056\n",
      "w_34 tensor([ 5.9858, -6.2510], requires_grad=True)\n",
      "w_012 tensor([-4.1268,  4.1074,  9.2940], requires_grad=True)\n",
      "b_0 tensor([-3.4396], requires_grad=True)\n",
      "b_1 tensor([-2.2386], requires_grad=True)\n",
      "epoch 6150; error=0.055\n",
      "w_34 tensor([ 5.9985, -6.2638], requires_grad=True)\n",
      "w_012 tensor([-4.1450,  4.1255,  9.3282], requires_grad=True)\n",
      "b_0 tensor([-3.4457], requires_grad=True)\n",
      "b_1 tensor([-2.2473], requires_grad=True)\n",
      "epoch 6200; error=0.054\n",
      "w_34 tensor([ 6.0109, -6.2763], requires_grad=True)\n",
      "w_012 tensor([-4.1628,  4.1434,  9.3620], requires_grad=True)\n",
      "b_0 tensor([-3.4518], requires_grad=True)\n",
      "b_1 tensor([-2.2559], requires_grad=True)\n",
      "epoch 6250; error=0.053\n",
      "w_34 tensor([ 6.0232, -6.2886], requires_grad=True)\n",
      "w_012 tensor([-4.1804,  4.1610,  9.3953], requires_grad=True)\n",
      "b_0 tensor([-3.4577], requires_grad=True)\n",
      "b_1 tensor([-2.2642], requires_grad=True)\n",
      "epoch 6300; error=0.052\n",
      "w_34 tensor([ 6.0352, -6.3007], requires_grad=True)\n",
      "w_012 tensor([-4.1978,  4.1784,  9.4281], requires_grad=True)\n",
      "b_0 tensor([-3.4636], requires_grad=True)\n",
      "b_1 tensor([-2.2726], requires_grad=True)\n",
      "epoch 6350; error=0.052\n",
      "w_34 tensor([ 6.0471, -6.3126], requires_grad=True)\n",
      "w_012 tensor([-4.2148,  4.1955,  9.4604], requires_grad=True)\n",
      "b_0 tensor([-3.4693], requires_grad=True)\n",
      "b_1 tensor([-2.2807], requires_grad=True)\n",
      "epoch 6400; error=0.051\n",
      "w_34 tensor([ 6.0587, -6.3243], requires_grad=True)\n",
      "w_012 tensor([-4.2317,  4.2124,  9.4924], requires_grad=True)\n",
      "b_0 tensor([-3.4750], requires_grad=True)\n",
      "b_1 tensor([-2.2888], requires_grad=True)\n",
      "epoch 6450; error=0.05\n",
      "w_34 tensor([ 6.0702, -6.3359], requires_grad=True)\n",
      "w_012 tensor([-4.2484,  4.2290,  9.5239], requires_grad=True)\n",
      "b_0 tensor([-3.4806], requires_grad=True)\n",
      "b_1 tensor([-2.2968], requires_grad=True)\n",
      "epoch 6500; error=0.049\n",
      "w_34 tensor([ 6.0815, -6.3472], requires_grad=True)\n",
      "w_012 tensor([-4.2648,  4.2454,  9.5549], requires_grad=True)\n",
      "b_0 tensor([-3.4861], requires_grad=True)\n",
      "b_1 tensor([-2.3047], requires_grad=True)\n",
      "epoch 6550; error=0.049\n",
      "w_34 tensor([ 6.0926, -6.3584], requires_grad=True)\n",
      "w_012 tensor([-4.2810,  4.2617,  9.5856], requires_grad=True)\n",
      "b_0 tensor([-3.4915], requires_grad=True)\n",
      "b_1 tensor([-2.3124], requires_grad=True)\n",
      "epoch 6600; error=0.048\n",
      "w_34 tensor([ 6.1035, -6.3694], requires_grad=True)\n",
      "w_012 tensor([-4.2970,  4.2776,  9.6159], requires_grad=True)\n",
      "b_0 tensor([-3.4968], requires_grad=True)\n",
      "b_1 tensor([-2.3201], requires_grad=True)\n",
      "epoch 6650; error=0.047\n",
      "w_34 tensor([ 6.1143, -6.3802], requires_grad=True)\n",
      "w_012 tensor([-4.3128,  4.2934,  9.6458], requires_grad=True)\n",
      "b_0 tensor([-3.5021], requires_grad=True)\n",
      "b_1 tensor([-2.3277], requires_grad=True)\n",
      "epoch 6700; error=0.047\n",
      "w_34 tensor([ 6.1249, -6.3909], requires_grad=True)\n",
      "w_012 tensor([-4.3284,  4.3089,  9.6753], requires_grad=True)\n",
      "b_0 tensor([-3.5072], requires_grad=True)\n",
      "b_1 tensor([-2.3351], requires_grad=True)\n",
      "epoch 6750; error=0.046\n",
      "w_34 tensor([ 6.1354, -6.4014], requires_grad=True)\n",
      "w_012 tensor([-4.3438,  4.3244,  9.7044], requires_grad=True)\n",
      "b_0 tensor([-3.5123], requires_grad=True)\n",
      "b_1 tensor([-2.3425], requires_grad=True)\n",
      "epoch 6800; error=0.045\n",
      "w_34 tensor([ 6.1456, -6.4118], requires_grad=True)\n",
      "w_012 tensor([-4.3590,  4.3395,  9.7332], requires_grad=True)\n",
      "b_0 tensor([-3.5173], requires_grad=True)\n",
      "b_1 tensor([-2.3498], requires_grad=True)\n",
      "epoch 6850; error=0.045\n",
      "w_34 tensor([ 6.1558, -6.4220], requires_grad=True)\n",
      "w_012 tensor([-4.3740,  4.3545,  9.7616], requires_grad=True)\n",
      "b_0 tensor([-3.5222], requires_grad=True)\n",
      "b_1 tensor([-2.3569], requires_grad=True)\n",
      "epoch 6900; error=0.044\n",
      "w_34 tensor([ 6.1658, -6.4320], requires_grad=True)\n",
      "w_012 tensor([-4.3888,  4.3693,  9.7897], requires_grad=True)\n",
      "b_0 tensor([-3.5271], requires_grad=True)\n",
      "b_1 tensor([-2.3641], requires_grad=True)\n",
      "epoch 6950; error=0.043\n",
      "w_34 tensor([ 6.1756, -6.4419], requires_grad=True)\n",
      "w_012 tensor([-4.4034,  4.3839,  9.8174], requires_grad=True)\n",
      "b_0 tensor([-3.5319], requires_grad=True)\n",
      "b_1 tensor([-2.3711], requires_grad=True)\n",
      "epoch 7000; error=0.043\n",
      "w_34 tensor([ 6.1853, -6.4517], requires_grad=True)\n",
      "w_012 tensor([-4.4179,  4.3983,  9.8448], requires_grad=True)\n",
      "b_0 tensor([-3.5367], requires_grad=True)\n",
      "b_1 tensor([-2.3781], requires_grad=True)\n",
      "epoch 7050; error=0.042\n",
      "w_34 tensor([ 6.1949, -6.4613], requires_grad=True)\n",
      "w_012 tensor([-4.4322,  4.4127,  9.8719], requires_grad=True)\n",
      "b_0 tensor([-3.5413], requires_grad=True)\n",
      "b_1 tensor([-2.3849], requires_grad=True)\n",
      "epoch 7100; error=0.042\n",
      "w_34 tensor([ 6.2043, -6.4708], requires_grad=True)\n",
      "w_012 tensor([-4.4463,  4.4267,  9.8986], requires_grad=True)\n",
      "b_0 tensor([-3.5460], requires_grad=True)\n",
      "b_1 tensor([-2.3916], requires_grad=True)\n",
      "epoch 7150; error=0.041\n",
      "w_34 tensor([ 6.2136, -6.4802], requires_grad=True)\n",
      "w_012 tensor([-4.4603,  4.4406,  9.9251], requires_grad=True)\n",
      "b_0 tensor([-3.5505], requires_grad=True)\n",
      "b_1 tensor([-2.3984], requires_grad=True)\n",
      "epoch 7200; error=0.041\n",
      "w_34 tensor([ 6.2228, -6.4894], requires_grad=True)\n",
      "w_012 tensor([-4.4741,  4.4544,  9.9512], requires_grad=True)\n",
      "b_0 tensor([-3.5550], requires_grad=True)\n",
      "b_1 tensor([-2.4050], requires_grad=True)\n",
      "epoch 7250; error=0.04\n",
      "w_34 tensor([ 6.2319, -6.4985], requires_grad=True)\n",
      "w_012 tensor([-4.4877,  4.4680,  9.9771], requires_grad=True)\n",
      "b_0 tensor([-3.5594], requires_grad=True)\n",
      "b_1 tensor([-2.4115], requires_grad=True)\n",
      "epoch 7300; error=0.04\n",
      "w_34 tensor([ 6.2409, -6.5075], requires_grad=True)\n",
      "w_012 tensor([-4.5012,  4.4815, 10.0027], requires_grad=True)\n",
      "b_0 tensor([-3.5638], requires_grad=True)\n",
      "b_1 tensor([-2.4180], requires_grad=True)\n",
      "epoch 7350; error=0.039\n",
      "w_34 tensor([ 6.2497, -6.5163], requires_grad=True)\n",
      "w_012 tensor([-4.5145,  4.4949, 10.0280], requires_grad=True)\n",
      "b_0 tensor([-3.5681], requires_grad=True)\n",
      "b_1 tensor([-2.4243], requires_grad=True)\n",
      "epoch 7400; error=0.039\n",
      "w_34 tensor([ 6.2584, -6.5251], requires_grad=True)\n",
      "w_012 tensor([-4.5276,  4.5080, 10.0530], requires_grad=True)\n",
      "b_0 tensor([-3.5723], requires_grad=True)\n",
      "b_1 tensor([-2.4306], requires_grad=True)\n",
      "epoch 7450; error=0.038\n",
      "w_34 tensor([ 6.2671, -6.5337], requires_grad=True)\n",
      "w_012 tensor([-4.5407,  4.5210, 10.0778], requires_grad=True)\n",
      "b_0 tensor([-3.5765], requires_grad=True)\n",
      "b_1 tensor([-2.4368], requires_grad=True)\n",
      "epoch 7500; error=0.038\n",
      "w_34 tensor([ 6.2755, -6.5422], requires_grad=True)\n",
      "w_012 tensor([-4.5535,  4.5339, 10.1023], requires_grad=True)\n",
      "b_0 tensor([-3.5806], requires_grad=True)\n",
      "b_1 tensor([-2.4430], requires_grad=True)\n",
      "epoch 7550; error=0.037\n",
      "w_34 tensor([ 6.2839, -6.5507], requires_grad=True)\n",
      "w_012 tensor([-4.5663,  4.5467, 10.1265], requires_grad=True)\n",
      "b_0 tensor([-3.5847], requires_grad=True)\n",
      "b_1 tensor([-2.4491], requires_grad=True)\n",
      "epoch 7600; error=0.037\n",
      "w_34 tensor([ 6.2922, -6.5590], requires_grad=True)\n",
      "w_012 tensor([-4.5789,  4.5593, 10.1504], requires_grad=True)\n",
      "b_0 tensor([-3.5888], requires_grad=True)\n",
      "b_1 tensor([-2.4552], requires_grad=True)\n",
      "epoch 7650; error=0.037\n",
      "w_34 tensor([ 6.3004, -6.5672], requires_grad=True)\n",
      "w_012 tensor([-4.5914,  4.5717, 10.1741], requires_grad=True)\n",
      "b_0 tensor([-3.5928], requires_grad=True)\n",
      "b_1 tensor([-2.4612], requires_grad=True)\n",
      "epoch 7700; error=0.036\n",
      "w_34 tensor([ 6.3085, -6.5753], requires_grad=True)\n",
      "w_012 tensor([-4.6038,  4.5840, 10.1976], requires_grad=True)\n",
      "b_0 tensor([-3.5967], requires_grad=True)\n",
      "b_1 tensor([-2.4671], requires_grad=True)\n",
      "epoch 7750; error=0.036\n",
      "w_34 tensor([ 6.3165, -6.5833], requires_grad=True)\n",
      "w_012 tensor([-4.6160,  4.5962, 10.2208], requires_grad=True)\n",
      "b_0 tensor([-3.6006], requires_grad=True)\n",
      "b_1 tensor([-2.4730], requires_grad=True)\n",
      "epoch 7800; error=0.035\n",
      "w_34 tensor([ 6.3243, -6.5912], requires_grad=True)\n",
      "w_012 tensor([-4.6281,  4.6083, 10.2438], requires_grad=True)\n",
      "b_0 tensor([-3.6045], requires_grad=True)\n",
      "b_1 tensor([-2.4788], requires_grad=True)\n",
      "epoch 7850; error=0.035\n",
      "w_34 tensor([ 6.3321, -6.5990], requires_grad=True)\n",
      "w_012 tensor([-4.6401,  4.6202, 10.2665], requires_grad=True)\n",
      "b_0 tensor([-3.6083], requires_grad=True)\n",
      "b_1 tensor([-2.4846], requires_grad=True)\n",
      "epoch 7900; error=0.035\n",
      "w_34 tensor([ 6.3398, -6.6067], requires_grad=True)\n",
      "w_012 tensor([-4.6519,  4.6321, 10.2891], requires_grad=True)\n",
      "b_0 tensor([-3.6120], requires_grad=True)\n",
      "b_1 tensor([-2.4903], requires_grad=True)\n",
      "epoch 7950; error=0.034\n",
      "w_34 tensor([ 6.3474, -6.6144], requires_grad=True)\n",
      "w_012 tensor([-4.6637,  4.6438, 10.3114], requires_grad=True)\n",
      "b_0 tensor([-3.6157], requires_grad=True)\n",
      "b_1 tensor([-2.4959], requires_grad=True)\n",
      "Final result:\n",
      " Input: [0. 0.]\n",
      " Output: 0.097\n",
      " Target: 0.0\n",
      "\n",
      "Final result:\n",
      " Input: [1. 0.]\n",
      " Output: 0.926\n",
      " Target: 1.0\n",
      "\n",
      "Final result:\n",
      " Input: [1. 1.]\n",
      " Output: 0.09\n",
      " Target: 0.0\n",
      "\n",
      "Final result:\n",
      " Input: [0. 1.]\n",
      " Output: 0.896\n",
      " Target: 1.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# New data D and labels y for xor\n",
    "D = np.zeros((4,2),dtype=float)\n",
    "D[0,:] = [0.,0.]\n",
    "D[1,:] = [0.,1.]\n",
    "D[2,:] = [1.,0.]\n",
    "D[3,:] = [1.,1.]\n",
    "D = torch.tensor(D,dtype=torch.float)\n",
    "Y_xor = torch.tensor([0.,1.,1.,0.])\n",
    "N = D.shape[0] # number of input patterns\n",
    "\n",
    "# Initialize parameters\n",
    "#     Although you will implement gradient descent manually, let's set requires_grad=True\n",
    "#     anyway so PyTorch will track the gradient too, and we can compare your gradient with PyTorch's.\n",
    "w_34 = torch.randn(2) # [size 2] tensor representing [w_3,w_4]\n",
    "w_012 = torch.randn(3) # [size 3] tensor representing [w_0,w_1,w_2]\n",
    "b_0 = torch.randn(1) # [size 1] tensor\n",
    "b_1 = torch.randn(1) # [size 1] tensor\n",
    "w_34.requires_grad=True\n",
    "w_012.requires_grad=True\n",
    "b_0.requires_grad=True\n",
    "b_1.requires_grad=True\n",
    "\n",
    "alpha = 0.05 # learning rate\n",
    "nepochs = 8000 # number of epochs\n",
    "\n",
    "track_error = []\n",
    "verbose = True\n",
    "for e in range(nepochs): # for each epoch\n",
    "    error_epoch = 0. # sum loss across the epoch\n",
    "    perm = np.random.permutation(N)\n",
    "    for p in perm: # visit data points in random order\n",
    "        x = D[p,:] # input pattern\n",
    "        \n",
    "        # Compute the output of hidden neuron h\n",
    "        # e.g., two lines like the following\n",
    "        net_h = torch.dot(x,w_34)+b_0\n",
    "        h = g_logistic(net_h)\n",
    "        # TODO : YOUR CODE GOES HERE\n",
    "        #raise Exception('Replace with your code.')                  \n",
    "        \n",
    "        # Compute the output of neuron yhat\n",
    "        # e.g., two lines like the following\n",
    "        \n",
    "        x_h = torch.cat((x,h))\n",
    "        net_y = torch.dot(x_h,w_012)+b_1\n",
    "        yhat = g_logistic(net_y)\n",
    "        \n",
    "        # TODO : YOUR CODE GOES HERE\n",
    "        #raise Exception('Replace with your code.')                     \n",
    "        \n",
    "        # compute loss\n",
    "        y = Y_xor[p]\n",
    "        myloss = loss(yhat,y)\n",
    "        error_epoch += myloss.item()\n",
    "        \n",
    "        # print output if this is the last epoch\n",
    "        if (e == nepochs-1):\n",
    "            print(\"Final result:\")\n",
    "            print_forward(x,yhat,y)\n",
    "            print(\"\")\n",
    "\n",
    "        # Compute the gradient manually\n",
    "        if verbose:\n",
    "            print('Compute the gradient manually')\n",
    "            print_forward(x,yhat,y)\n",
    "        with torch.no_grad():\n",
    "            # TODO : YOUR GRADIENT CODE GOES HERE\n",
    "            #  should include at least these 4 lines (helper lines may be useful)\n",
    "            w_34_grad = torch.tensor((  (2 * (yhat - y) * g_logistic(net_y) * (1 - g_logistic(net_y)) * w_012[2]) * (g_logistic(net_h) * (1 - g_logistic(net_h)) * x[0])   ,    (2 * (yhat - y) * g_logistic(net_y) * (1 - g_logistic(net_y)) * w_012[2]) * (g_logistic(net_h) * (1 - g_logistic(net_h)) * x[1])   ))                   \n",
    "            b_0_grad =    (2 * (yhat - y) * g_logistic(net_y) * (1 - g_logistic(net_y)) * w_012[2]) * (g_logistic(net_h) * (1 - g_logistic(net_h)) * 1)\n",
    "            \n",
    "            w_012_grad = torch.tensor((  ( 2 * (yhat - y) * g_logistic(net_y) * (1 - g_logistic(net_y)) * x_h[0] )    ,   ( 2 * (yhat - y) * g_logistic(net_y) * (1 - g_logistic(net_y)) * x_h[1] )    ,     ( 2 * (yhat - y) * g_logistic(net_y) * (1 - g_logistic(net_y)) * x_h[2] )      ))\n",
    "            b_1_grad = ( 2 * (yhat - y) * g_logistic(net_y) * (1 - g_logistic(net_y)) * 1 )\n",
    "\n",
    "            \n",
    "            #  make sure to inclose your code in the \"with torch.no_grad()\" wrapper,\n",
    "            #   otherwise PyTorch will try to track the \"gradient\" of the gradient computation, which we don't want.\n",
    "            #raise Exception('Replace with your code.')                      \n",
    "        if verbose:\n",
    "            print(\" Grad for w_34 and b_0\")\n",
    "            print_grad(w_34_grad.numpy(),b_0_grad.numpy())\n",
    "            print(\" Grad for w_012 and b_1\")\n",
    "            print_grad(w_012_grad.numpy(),b_1_grad.numpy())\n",
    "            print(\"\")\n",
    "\n",
    "        # Compute the gradient with PyTorch and compre with manual values\n",
    "        if verbose: print('Compute the gradient using PyTorch .backward()')\n",
    "        myloss.backward()\n",
    "        if verbose:\n",
    "            print(\" Grad for w_34 and b_0\")\n",
    "            print_grad(w_34.grad.numpy(),b_0.grad.numpy())\n",
    "            print(\" Grad for w_012 and b_1\")\n",
    "            print_grad(w_012.grad.numpy(),b_1.grad.numpy())\n",
    "            print(\"\")\n",
    "        w_34.grad.zero_() # clear PyTorch's gradient\n",
    "        b_0.grad.zero_()\n",
    "        w_012.grad.zero_()\n",
    "        b_1.grad.zero_()\n",
    "        \n",
    "        # Parameter update with gradient descent\n",
    "        with torch.no_grad():\n",
    "            # TODO : YOUR PARAMETER UPDATE CODE GOES HERE\n",
    "            # Four lines of the form\n",
    "            w_34 -= w_34_grad * alpha\n",
    "            b_0 -= b_0_grad * alpha\n",
    "            w_012 -= w_012_grad * alpha\n",
    "            b_1 -= b_1_grad * alpha\n",
    "            #raise Exception('Replace with your code.')\n",
    "            \n",
    "    if verbose==True: verbose=False\n",
    "    track_error.append(error_epoch)\n",
    "    if e % 50 == 0:\n",
    "        print(\"epoch \" + str(e) + \"; error=\" +str(round(error_epoch,3)))\n",
    "        print(\"w_34\", w_34 )\n",
    "        print(\"w_012\", w_012 )\n",
    "        print(\"b_0\", b_0 )\n",
    "        print(\"b_1\", b_1 )\n",
    "    \n",
    "# track output of gradient descent\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(track_error)\n",
    "plt.title('stochastic gradient descent (XOR)')\n",
    "plt.ylabel('error for epoch')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 5 (10 points) </h3>\n",
    "<br>\n",
    "After running your XOR network, print the values of the learned weights and biases. Your job now is to describe the solution that the network has learned. How does it work? Walk through each input pattern to describe how the network computes the right answer (if it does). See discussion in lecture for an example.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: The network learned these weights and biases through the forward and backpropagation process of a simple multi-layer network. By having a hidden layer, weights w0 to w4 and biases b0 to b1 are updated by subtracting the (gradient x alpha). The logistic function compares the yhat expection with the y true value and adjusts these weights and biases based the calculated gradients. This process is repeated for 8000 epochs which leads to the error = ~0.04. \\\n",
    "\n",
    "There exist 4 different cases for input pattern <x0, x1>. Also, at each neurons we use logistic function to determine 0 if negative and 1 if positive.\n",
    "\n",
    "1) <0,0> When both inputs are 0, then x0 and x1 do not turn on, which makes h only depends on b0 = -3.6193. Since b0 is negative, h turns off, and yhat only depends on b1=-2.5014. As a result, yhat=0.\n",
    "\n",
    "2) <0,1> Having only x1=1, h = x1 * w4 + b0 = -6.6217 -3.6193 = -10.241. h is negative so it turns off. Then, yhat = w1 + b1 = 4.6552 -2.5014 = 2.1538, which means yhat=1.\n",
    "\n",
    "3) <1,0> Having only x0=1, h = w3 + b0 = 6.3548 -3.6193 = 2.7355; this turns on h=1. Then, yhat = w2 + b1 + w0 = 10.3330 -2.5014 -4.6750 = 3.1566, this turns on the neuron yhat = 1.\n",
    "\n",
    "4) <1,1> Having both x1 = x0 = 1, h = w3 + w4 + b0 = 6.3548 -6.6217 -3.6193 = -3.8862, turning off h = 0. Then, yhat = w0 + w1 + b1 = -4.6750 + 4.6552 -2.5014 = -2.5212, resulting in yhat = 0, as it is a negative value in the logistic function.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_34 tensor([ 6.3548, -6.6217], requires_grad=True)\n",
      "w_012 tensor([-4.6750,  4.6552, 10.3330], requires_grad=True)\n",
      "b_0 tensor([-3.6193], requires_grad=True)\n",
      "b_1 tensor([-2.5014], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"w_34\", w_34 )\n",
    "print(\"w_012\", w_012 )\n",
    "print(\"b_0\", b_0 )\n",
    "print(\"b_1\", b_1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# talk about network ran correctly\n",
    "# w_3, w_4 being pos/neg\n",
    "# understand what the model is doing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
